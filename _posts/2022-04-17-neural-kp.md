---
layout: article
title: 神经网络求解背包问题 
key: neural-kp
tags: KP
category: blog
pageview: true
date: 2022-04-17 10:00:00 +08:00
---

在一些数据科学的例子中, 需要对模型的输出运行特定的算法才能得到结果. 有时比较简单, 就像找到最大输出的索引; 而有时我们需要更高级的算法. 我们可能在推断之后再运行算法. 但是, 设计一个可以内部运行的算法确实有着它独特的优势. 使用神经网络求解背部问题不但帮助模型在内部运行相关的算法, 还允许模型的端到端训练.

# 什么是背包问题 (Knapsack Problem, KP) ?

我们可以通过一个有趣的小故事来理解什么是 KP. 故事中有一个小偷, 他有一些东西可以偷. 每个物品有它自己的 重量 (weight) 和 价值 (prize). 而我们可怜的小偷只有一个背包, 这个背包的容量 (capacity) 代表了 **小偷能偷的物品的最大数量**. 成功之后, 我们可以通过对偷取物理的价值求和来求得小偷的利润. 我们的目标是最大化这一利润的同时, 保证选择物品的重量总和不会超过背包的容量.

# 数据
要使用神经网络求解 KP, 第一步是准备数据. 对于物品的重量, 价值, 和背包的容量, 我们都使用随机的整数来表示. 在创建这个问题之后, 我们可以使用精确算法 brute force 来找到最优解:
```
import numpy as np


def brute_force_kp(x_weights, x_prices, x_capacity):
    # 物品的总数量
    item_count = x_weights.shape[0]
    # 选择空间 (总共有 2^n 次选择)
    pick_spaces = 2 ** item_count
    # 初始化记录变量
    best_price = -1
    best_picks = np.zeros(item_count)
    for p in range(pick_spaces):
        # 以2进制递进的方式表示每一次选择. 比如:
        # 我们有5个物品, 第一什么都不选: [0 0 0 0 0]
        # 我们有5个物品, 第二只选最后一个: [0 0 0 0 1]
        # 以此类推 ...
        picks = [int(c) for c in f"{p:0{item_count}b}"]
        price = np.dot(x_prices, picks)
        weight = np.dot(x_weights, picks)
        if weight <= x_capacity and price > best_price:
            best_price = price
            best_picks = picks
    return best_picks

def create_kp(item_count=5):
    x_weights = np.random.randint(1, 45, item_count)
    x_prices = np.random.randint(1, 99, item_count)
    x_capacity = np.random.randint(1, 99)
    y = brute_force_kp(x_weights, x_prices, x_capacity)
    print("Weights: ", x_weights)
    print("Prices: ", x_prices)
    print("Capacity: ", x_capacity)
    print("Optimal Solution: ", y)
    return x_weights, x_prices, x_capacity, y   

x_weights, x_prices, x_capacity, y = create_kp() 
```

输出结果:
```
Weights:  [43 14  7  4 20]
Prices:  [37 18 61 16 81]
Capacity:  66
Optimal Solution:  [0, 1, 1, 1, 1]
```

有了生成训练数据的函数之后, 我们就需要考虑模型输入的预处理了. 归一化输入是每个机器学习项目的一部分, 因为它帮助模型更好的一般化. 要归一化每个背包问题的实例:

* 用每个实例的最大价值除以每个物品的单价
* 用每个实例的背包容量除以每个物品的重量
* 从输入中移除容量这一信息因为它已经被嵌入到重量这一变量中去了

所以归一化后, 最后上述实例的数据应该如下:
```
Weights:  [0.65151515 0.21212121 0.10606061 0.06060606 0.3030303 ]
Prices:  [0.45679012 0.22222222 0.75308642 0.19753086 1.        ]
Optimal Solution:  [0, 1, 1, 1, 1]
```

在本教程中, 我们生成包含一万样本数量的训练集, 200个测试集.

## 模型

我们选择的模型是比较简单的模型. 他有两个输入和一个输出. 最开始时, 输入被合并到一起, 这一结果进入两个由 sigmoid 激活的连续密集层. 第二密集层的输出也就是模型的输出. 我们可以使用 Keras 实现模型:  
```
import tensorflow as tf


def get_model(item_count=5):
    input_weights = tf.keras.Input((item_count, ))
    input_prices = tf.keras.Input((item_count, ))
    inputs_concat = tf.keras.Concatenate(name="Concatenate")([input_weights, input_prices])
    picks = tf.keras.layers.Dense(
        item_count ** 2 + item * 2, activation="sigmoid", name="Hidden")(input_concat)
    picks = tf.keras.layers.Dense(
        item_count, activation="sigmoid", name="Output")(pick)
    model = tf.keras.Model(inputs=[input_weights, input_prices], outputs=[picks])
    return model
```

下图可视化了模型的架构:

![Model Framework](/fig/neural-kp-model.jpg)

# 度量标准 (Metrics)

## Over Pricing
要评估性能, 背包问题需要具体的度量标准而不仅仅是简单的二元分类准确度. 我们第一个要介绍的度量标准是 **在价值上进行评估**, 它评估了选择物品价值与最优解价值之间的平均差. Keras 框架下我们可以这么实现:
```
import tensorflow.keras.backend as backend


def metric_overprcing(input_prices):
    def _overpricing(y_true, y_pred):
        y_pred = backend.round(y_pred)
        res = backend.mean(
            backend.batch_dot(y_pred, input_prices, 1) - backend.batch_dot(y_true, input_prices, 1))
        return res

    return _overpricing
```

## Over Space Violation
另一个重要的度量标准是使用的容量是否超过背包的容量, 也就是在 *超出背包的容量* 上进行评估, 具体是已选物品重量与背包容量的差. 实现如下:
```
def metric_space_violation(input_weights):
    def _space_violation(y_true, y_pred):
        y_pred = backend.round(y_pred):
        res = backend.mean(
            backend.maximum(backend.batch_dot(y_pred, input_weights, 1) - 1, 0))
        return res
    return _space_violation
```

# Over Pick Count
最后, 我们还可以设置一个度量标准 - 也就是选择物品的个数与最优解的差距:
```
def metric_pick_count():
    def _pick_count(y_true, y_pred):
        y_pred = backend.round(y_pred)
        return backend.mean(backend.sum(y_pred, -1))

    return _pick_count
```

注意, 上述三个度量标准返回的是函数, 这是因为 tensorflow 内部会在调用时自动填充 `y_true` 与 `y_pred`.

# 监督学习

首先我们可以用监督学习的方法去求解 KP, 我们输入重量和价值到模型中, 并估计最优值作为输出. 在这一过程中, 模型训练使用交叉熵损失函数, 完整代码如下:
```
def supervised_model(item_count=5):
    input_weights = tf.keras.Input((item_count, ))
    input_prices = tf.keras.Input((item_count, ))
    inputs_concat = tf.keras.Concatenate(name="Concatenate")([input_weights, input_prices])
    picks = tf.keras.layers.Dense(
        item_count ** 2 + item * 2, activation="sigmoid", name="Hidden")(input_concat)
    picks = tf.keras.layers.Dense(
        item_count, activation="sigmoid", name="Output")(pick)
    model = tf.keras.Model(inputs=[input_weights, input_prices], outputs=[picks])
    model.compile("adam", binary_crossentropy,
                  metrics=[binary_accuracy, metric_space_violation(input_weights),
                           metric_overprice(input_prices)])
    return model
```

接下来是训练函数:
```
import os
import tensorflow.keras.callbacks as callbacks

def train(model, train_x, train_y, test_x, test_y):
    if os.path.exists("best_model.h5"): os.remove("best_model.h5")
    model.fit(train_x, train_y, epochs=512, verbose=1, 
              callbacks=[callbacks.ModelCheckPoint("best_model.h5", monitor="binary_accurac",
                                                   save_best_only=True, save_weights_only=True)])
    model.load_weights("best_model.h5")
    train_results = model.evaluate(train_x, train_y, 64, 0)
    test_results = model.evaluate(test_x, test_y, 64, 0)
    print("Model results(Train/Test):")
    print(f"Loss:               {train_results[0]:.2f} / {test_results[0]:.2f}")
    print(f"Binary accuracy:    {train_results[1]:.2f} / {test_results[1]:.2f}")
    print(f"Space violation:    {train_results[2]:.2f} / {test_results[2]:.2f}")
    print(f"Overpricing:        {train_results[3]:.2f} / {test_results[3]:.2f}")
    print(f"Pick count:         {train_results[4]:.2f} / {test_results[4]:.2f}")    
```

训练512个 epoch 之后, 我们可以得到如下的结果:
```
Model results(Train/Test):
Loss:               0.24 / 0.24
Binary accuracy:    0.89 / 0.89
Space violation:    0.04 / 0.06
Overpricing:        0.06 / 0.08
```

不幸的是, 监督学习有两个基础性的问题:

* 要开始训练, 最优解是必须的
* 在 *space violation* 和 *pricing* 的度量标准中, 我们没有任何控制.

# 无监督方法
我们想要在没有最优解的情况下训练模型. 因此, 我们需要一个新的损失函数. 相对应地, 我们分别定义 $w, p, c$ 和 $o$ 维物品的重量, 物品的价值, 背包的容量以及模型的输出. 首先, 我们想要最大化所选物品的价值之和. 计算如下:

$$\mathcal P = p \cdot o$$

同时, 我们还想要选择的重量不能超过背包的容量. 则:

$$\mathcal W = \max (w \cdot o - c, 0)$$

最后, 我们的两个目标 - 最大化 $\mathcal P$ 的同时最小化 $\mathcal W$, 可以用来构建如下的损失函数:

$$\mathcal L = \mathcal W - \mathcal P$$

注意因为我们要最小化损失, 所以符号是反着的. 为了控制超出重量的权重, 我们可以在第一项添加一个系数 $\lambda$, 最终的损失函数如下:
```
def kp_loss(input_weights, input_prices, input_capacity, cvc=1):
    def _loss(y_true, y_pred):
        picks = y_pred
        violation = backend.maximum(backend.batch_dot(picks, input_weights, 1) - 1, 0)
        price = backend.batch_dot(picks, input_prices, 1)
        return cvc * violation - price

    return _loss
```

通过这一损失函数, 我们可以无监督训练网络. 需要注意的是, 尽管我们接受最优解 `y_true` 作为函数的参数, 我们并不使用它来计算损失. 但是我们并不能避免这一参数因为它是 Keras 的一个限制, 我们需要同时在损失函数中接受预测和期望的输出. 另外, 背包的容量是1, 因为我们归一化了数据.

最终, 无监督训练的模型如下:
```
def unsupervised_model(cvc=5.75, item_count=5):
    input_weights = tf.keras.Input((item_count, ))
    input_prices = tf.keras.Input((item_count, ))
    inputs_concat = tf.keras.layers.Concatenate()([input_weights, input_prices])
    picks = tf.keras.layers.Dense(item_count**2 + item_count*2, activation="sigmoid")(inputs_concat)
    picks = tf.keras.layers.Dense(item_count, activation="sigmoid")(picks)
    model = tf.keras.Model(inputs=[input_weights, input_prices], outputs=[picks])
    model.compile("adam", kp_loss(input_weights, input_prices, cvc), 
                  metrics=[tf.keras.metrics.binary_accuracy, metric_space_violation(input_weights),
                           metric_overprice(input_prices), metric_pick_count()])
    return model
```

在使用无监督方法训练512个 epoch 之后, 我们会得到如下相似的结果:
```
Model results(Train/Test):
Loss:               -1.83 / -1.75
Binary accuracy:    0.89 / 0.89
Space violation:    0.05 / 0.06
Overpricing:        0.12 / 0.14
```

提升 $\lambda$ 的值会导致更少的超出空间和更少的价值总和. 如果增加过多也会导致二元分类准确率下降. 经过测试, 一个比较合适的值是5.75.

# 总结
正如之前讨论的, 当梯度可以在网络中传递时, 我们使用神经网络来求解背包问题. 所以我们的模型可以被端到端训练, 使用任何后向传播算法. 这种方式可以被用于推荐系统中, 尤其是当空间利用率固定时推荐多个物品 (像是横幅广告等).

---

原博文链接: https://towardsdatascience.com/neural-knapsack-8edd737bdc15

